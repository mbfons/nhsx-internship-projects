---
remote_theme: nhsx/nhs-io-theme
title: Medical Visual Question Answering Approaches and Datasets
description: NHS England PhD Internship - Medical Visual Question Answering Approaches and Datasets
permalink: /visual-question-answer/
---

# Medical Visual Question Answering Approaches and Datasets

**Keywords:**  NLP, VQA, MultiModalData

**Need:**  Visual Question Answering (VQA) is an interesting challenge combining different disciplines, including computer vision, natural language understanding, and deep learning techniques. VQA in the medical domain incorporates areas such as diagnosis, helping patients understand their medical conditions, and answering the corresponding questions accurately in unlabeled datasets.

Further, when considering Medical VQA we have to be aware that images and their descriptions generated in the healthcare sector are often very different from images and text taken from the general domain.  Thus there is a gap between many pre-trained models which could be useful and this problem.  There is also a large cost associated with finding healthcare professionals with suitable expertise to annotate these images as well.  This is also a challenging area in which to evaluate success, due to the need to understand how to constrain the evaluation space, whilst not stifling the need for flexibility in the approach.

The project would look to understand the current state of Medical VQA, the available datasets and challenges, and the ability of current approaches to this task.  The interpretability and explainability of technical approaches would be of interest to consider.

**Current Knowledge/Examples & Possible Techniques/Approaches:**  
- [Medical Visual Question Answering: A Survey](https://arxiv.org/abs/2111.10056)
- [BPI-MVQA: a bi-branch model for medical visual question answering](https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-022-00800-x)
- [Overview of the VQA-Med Task at ImageCLEF 2020: Visual Question Answering and Generation in the Medical Domain](http://ceur-ws.org/Vol-2696/paper_106.pdf)
- [MMBERT: Multimodal BERT Pretraining for Improved Medical VQA](https://arxiv.org/abs/2104.01394)
- [Learning to Ask Like a Physician](https://arxiv.org/abs/2206.02696)
- [https://visualqa.org/](https://visualqa.org/)


**Related Previous Internship Projects:** [TxtRayAlign](https://nhsx.github.io/nhsx-internship-projects/text-description-imaging/)

**Enables Future Work:**  Demonstration and deeper understanding of VQA as a component of explainability in medical imaging and working with multi-modal datasets.

**Outcome/Learning Objectives:**   Alongside a working example of a VQA model on a medical dataset, the project would hope to build an understanding of the state of the field of VQA and the challenges specific to the healthcare domain.

**Datasets:** 
- [Visual Question Answering in the Medical Domain VQA-Med 2019](https://github.com/abachaa/VQA-Med-2019)
- [Visual Question Answering in the Medical Domain | ImageCLEF / LifeCLEF - Multimedia Retrieval in CLEF](https://www.imageclef.org/2020/medical/vqa)
- [ImageCLEF-VQAMed | ImageCLEF / LifeCLEF - Multimedia Retrieval in CLEF](https://www.imageclef.org/2021/medical/vqa)


**Desired skill set:**  When applying please highlight any experience around computer vision, natural language processing, deep learning, coding experience (including any coding in the open), and any other data science experience you feel relevant.


---
Return to list of [all available projects](https://nhsx.github.io/nhsx-internship-projects/).
